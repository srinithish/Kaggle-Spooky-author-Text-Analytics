{
    "collab_server" : "",
    "contents" : "library\n?datatable\nlibrary(tidyverse)\n\nlibrary(DT)\n# library('wordcloud')\n# library(igraph)\n# library(ggraph)\n\n# \nlibrary(caret)\nlibrary(dtplyr)\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(ggplot2)\n\n###for text analytics\nlibrary(koRpus)\nlibrary(SnowballC)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(tm)\nlibrary(wordnet)\nlibrary(wordcloud)\nlibrary(nnet)\nlibrary(mxnet)\nunzip(file.choose())\ntrainOrig = fread(file.choose())\ntestOrig = fread(file.choose())\nView(head(trainOrig))\nsummary(trainOrig)\n\n# id                text              author         \n# Length:19579       Length:19579       Length:19579      \n# Class :character   Class :character   Class :character  \n# Mode  :character   Mode  :character   Mode  :character \n\nglimpse(trainOrig)\n\n\nVariables: 3\n# $ id     <chr> \"id26305\", \"id17569\", \"id11008\", \"id27763\", \"id12958\", \"id2296...\n# $ text   <chr> \"This process, however, afforded me no means of ascertaining t...\n# $ author <chr> \"EAP\", \"HPL\", \"EAP\", \"MWS\", \"HPL\", \"MWS\", \"EAP\", \"EAP\", \"EAP\",...\n\n\ntrainOrig %>% distinct(author)\n# \n# author\n# 1:    EAP\n# 2:    HPL\n# 3:    MWS\n\n# microbenchmark(nchar(trainOrig$text),str_length(trainOrig$text),stri_length(trainOrig$text),str_count(trainOrig$text))\ntrainOrig = trainOrig %>% mutate(len = stri_length(text))\ntestOrig = testOrig %>% mutate(len = stri_length(text))\n\n\ntrainOrig %>% group_by(author) %>% \n      summarise(MedianByAuthor = median(len)) %>%\n      arrange(MedianByAuthor) %>% ggplot(mapping = aes(x=author,y=MedianByAuthor))+\n      geom_bar(stat = \"identity\",fill=\"blue\") +\n      geom_text(mapping = aes(x = author,y= MedianByAuthor,label = MedianByAuthor),\n                color = \"orange\",nudge_y = 5)\n\n        \ntrainMutated = trainOrig %>% unnest_tokens(Words,text)\ntestMutated = testOrig %>% unnest_tokens(Words,text)\nas.tibble(trainMutated)\n\n# initDict()\n# synonyms(\"Understand\",\"VERB\")\n# getLemma(\"Understanding\")\n# \n# \n# filter = getTermFilter(\"StartsWithFilter\", \"Under\", TRUE)\n# terms = getIndexTerms(\"NOUN\",)\n# sapply(terms, getLemma)\n# class(terms)\n# str(terms)\n# getDict()\n# setDict(\"C://Program Files (x86)//WordNet//2.1//dict\")\n\ncountVsWordlength = trainMutated %>% group_by(id,author) %>% summarise(count = n()) %>% \n  ggplot()+geom_histogram(mapping = aes(x=count),binwidth = 10)+xlim(c(0,150))+facet_wrap(~author)+\n  xlab(\"Word length\")\nggsave(\"countVsWordlength.jpg\",plot = countVsWordlength)\n\n\n###TODO: word cloud for each author put this in a loop with filter for each author\ntrainMutated = trainMutated %>% as.tibble() %>% \n  anti_join(stop_words,by = c(\"Words\" = \"word\")) %>% \n  group_by(Words,author) %>% \n   summarise(wordcount = n()) %>% \n   arrange(desc(wordcount)) %>% head(30)%>% with(wordcloud(Words,wordcount,max.words=10)) \n# \n#  Words author wordcount\n#  <chr>  <chr>     <int>\n#    1    life    MWS       329\n#  2    love    MWS       273\n#  3   heart    MWS       262\n# trainOrig[id == \"id00001\",]  \n \n#  trainMutated = trainMutated %>% left_join(trainMutated %>% group_by(id,author,Words) %>% \n#    summarise(Wordcount = n()) %>% ungroup(),by = c(\"id\" = \"id\",\"author\" = \"author\",\"Words\" = \"Words\"))\n# \n#  trainMutated %>% bind_tf_idf(Words,id,Wordcount) %>% View()\n# \n# max(trainMutated$len)\n# trainMutated[id== \"id27184\",] %>% group_by(Words,Wordcount) %>% filter(Words == \"a\") \n#   summarise(sumterms = sum(Wordcount)) %>% filter(Words == \"a\") %>% mutate(tf = freq/Wordcount) %>%\n#   arrange(Words) %>% filter(Words == \"i\")\n#   \n# nrow(trainMutated[id== \"id27184\",] )\n\n\ntrainMutTFIDF = trainMutated %>% group_by(author,id,Words) %>% \n  summarise(Wordcount = n()) %>% \n    bind_tf_idf(Words,id,Wordcount) \n\n\ntrainMutTFIDF %>% ungroup() %>%  arrange(desc(tf_idf)) %>% View()\n\n\n# Checking the tfidf\n# trainOrig %>% distinct(id) %>% summarise(n())\n# View(stop_words)\n# 19579\n# trainMutated %>% filter(Words == \"mountebanks\") \n# log(19579/1)\n\n# data(\"AssociatedPress\", package = \"topicmodels\")\n# ap_td <- tidy(AssociatedPress)\n# # the output here ap_td should look very very similar to your tibble above\n# ap_td %>% cast_dtm(document, term, count)\n\nrm(df)\n?count\n\n  train_dtm <- trainOrig[,-c(\"len\",\"author\")] %>% head(1000) %>% \n    unnest_tokens(Words, text) %>% as.tibble() %>% \n    mutate(Words = stemDocument(Words)) %>% \n    group_by(id,Words) %>% summarise(n = n()) %>% ungroup() \n  \n  \ncastDTM = train_dtm %>% arrange(id)%>% cast_dtm(id,Words,n)\n  View(castDTM)\n  as.matrix(castDTM)[ ,1:10]\n  train_dtm =  trainMutated %>% mutate(Words = stemDocument(Words))\n\n    testDf = trainMutated %>% as.data.frame()\n    testDf[, -c(\"len\")]\n  \n dataForTrain = cbind(as.data.frame(as.matrix(castDTM)),as.data.frame(trainOrig[1:1000,c(\"author\")]))  \nhead(dataForTrain)\ncolnames(dataForTrain) = c(colnames(dataForTrain[,1:4363]),\"MainAuthor\")\n\n\nspooky.nnet = nnet(MainAuthor~., data = dataForTrain, size=1, maxit=500)\n  \n  \n  \n  ",
    "created" : 1509642104694.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3918391016",
    "id" : "8A713C29",
    "lastKnownWriteTime" : 1514123606,
    "last_content_update" : 1514123606889,
    "path" : "M:/Data analytics/My R Projects/Kaggle_SpookyAuthor/EDASpooky.R",
    "project_path" : "EDASpooky.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}