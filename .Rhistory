install.packages("tidytext")
install.packages("tidyverse")
?datatable
library(syuzhet)
library(DT)
install.packages("DT")
library(caret)
unzip(file.choose())
unzip(choose.files())
unzip(choose.file())
unzip(file.choose())
library(dtplyr)
library(dplyr)
library(data.table)
trainOrig = fread(file.choose())
testOrig = fread(file.choose())
head(train)
head(trainOrig)
library(caret)
view(head(trainOrig))
veiw(head(trainOrig))
View(trainOrig)
View(head(trainOrig))
summary(trainOrig)
str(trainOrig)
glimpse(trainOrig)
trainOrig %>% distinct(author)
library(stringr)
fruit <- c("apple", "banana", "pear", "pineapple")
str_count(fruit, "a")
library(stringi)
install.packages("microbenchmark")
library("microbenchmark", lib.loc="~/R/win-library/3.4")
microbenchmark(nchar(trainOrig$text),str_length(trainOrig$text),stri_length(trainOrig$text),str_count(trainOrig$text))
trainOrig %>% mutate(len = stri_length(text))
trainOrig = trainOrig %>% mutate(len = stri_length(text))
testOrig = testOrig %>% mutate(len = stri_length(text))
library(DT)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>% ungroup()
?order
order(c(2,3,1))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
ungroup() %>% (function(x){x[order(MedianByAuthor),]})
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
ungroup() %>% order_by(MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
ungroup() %>% order_by(MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
%>% order_by(MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(as.numeric(MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(as.numeric(MedianByAuthor)
)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(as.numeric(MedianByAuthor))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(as.double(MedianByAuthor))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(as.double(MedianByAuthor))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(as.double(MedianByAuthor))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(.MedianByAuthor)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
order_by(MedianByAuthor,author)
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
mutaute(author = order_by(MedianByAuthor,author))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
mutate(author = order_by(MedianByAuthor,author))
trainOrig %>% group_by(author) %>% summarise(MedianByAuthor = median(len)) %>%
arrange(MedianByAuthor)
trainMutated = trainOrig %>% unnest_tokens(Words,text)
library(DT)
# library('wordcloud')
# library(igraph)
# library(ggraph)
#
library(caret)
library(dtplyr)
library(dplyr)
library(data.table)
library(ggplot2)
###for text analytics
library(koRpus)
library(SnowballC)
library(tidytext)
library(stringr)
library(stringi)
library(tm)
library(wordnet)
library(wordcloud)
trainMutated = trainOrig %>% unnest_tokens(Words,text)
testMutated = testOrig %>% unnest_tokens(Words,text)
trainMutated %>% group_by(id,author,Words) %>%
summarise(Wordcount = n()) %>% bind_tf_idf(Words,id,Wordcount) %>% View()
nrow(trainMutated[id== "id27184",] )
trainMutTFIDF = trainMutated %>% group_by(id,author,Words) %>%
summarise(Wordcount = n()) %>%
bind_tf_idf(Words,id,Wordcount) %>%
View()
trainMutTFIDF = trainMutated %>% group_by(id,author,Words) %>%
summarise(Wordcount = n()) %>%
bind_tf_idf(Words,id,Wordcount)
trainMutTFIDF %>% View()
trainMutTFIDF %>% top_n(20,tf_idf)
trainMutTFIDF %>% top_n(20,"tf_idf")
trainMutTFIDF
trainMutTFIDF %>% top_n(20,tf_idf)
trainMutTFIDF %>% top_n(20,.$tf_idf)
trainMutTFIDF %>% top_n(20)
trainMutTFIDF %>% top_n(20)
trainMutTFIDF %>% View()
trainMutTFIDF %>% arrange(tf_idf)
trainMutTFIDF %>% arrange(desc(tf_idf))
mtcars
mtcars %>% arrange(desc(cyl))
trainMutTFIDF %>% arrange(desc(tf_idf))
trainMutTFIDF %>% arrange(desc(idf))
mtcars %>% arrange(desc(cyl),.by_group = FALSE)
mtcars %>% arrange(desc(cyl), .by_group = FALSE)
mtcars %>% arrange(desc(cyl), by_group = FALSE)
trainMutTFIDF %>% arrange(desc(idf),by_group = FALSE)
trainMutTFIDF %>% arrange(desc(tf_idf))
trainMutTFIDF %>% ungroup() %>% arrange(desc(tf_idf))
trainMutTFIDF %>%  ungroup() %>% arrange(id,desc(tf_idf))
install.packages("leaflet")
df = data.frame(
lat = rnorm(100),
lng = rnorm(100),
size = runif(100, 5, 20),
color = sample(colors(), 100)
)
m = leaflet(df) %>% addTiles()
m %>% addCircleMarkers(radius = ~size, color = ~color, fill = FALSE)
library(leaflet)
m = leaflet(df) %>% addTiles()
m %>% addCircleMarkers(radius = ~size, color = ~color, fill = FALSE)
m %>% addCircleMarkers(radius = ~size, color = ~color, fill = TRUE)
?addCircles
m %>% addCircleMarkers(radius = runif(100, 4, 10), label = "hi",color = c('red'))
m %>% addCircleMarkers(radius = runif(100, 4, 10), label = c("hi"),color = c('red'))
library(tidyverse)
library(DT)
# library('wordcloud')
# library(igraph)
# library(ggraph)
#
library(caret)
library(dtplyr)
library(dplyr)
library(data.table)
library(ggplot2)
###for text analytics
library(koRpus)
library(SnowballC)
library(tidytext)
library(stringr)
library(stringi)
library(tm)
library(wordnet)
library(wordcloud)
trainOrig$id %>% head() %>% gsub("(\\Qid\\E)","",.)
trainOrig$id %>% gsub("(\\Qid\\E)","",.) %>% length()
trainOrig$id
trainOrig$id %>% gsub("(\\Qid\\E)","",.) %>% distinct() %>% length()
trainOrig$id %>% gsub("(\\Qid\\E)","",.) %>% distinct() %>% length()
trainOrig$id %>% gsub("(\\Qid\\E)","",.) %>%
as.numeric() %>%
distinct() %>%
length()
trainOrig$id %>% gsub("(\\Qid\\E)","",.) %>%as.data.frame() %>%
distinct() %>%
length()
trainOrig$id %>% gsub("(\\Qid\\E)","",.) %>%as.data.frame() %>%
distinct() %>%
nrow()
trainOrig$id %>% length()
train_dtm$id = gsub("(\\Qid\\E)","",train_dtm$id)
train_dtm <- trainOrig[,-c("len")] %>% head(1000) %>%
unnest_tokens(Words, text) %>%
mutate(Words = stemDocument(Words)) %>%
group_by(author,id,Words) %>% summarise(n = n()) %>% ungroup()
train_dtm$id = gsub("(\\Qid\\E)","",train_dtm$id)
train_dtm %>% cast_dtm(id,Words,n)
train_dtm$id = as.numeric(gsub("(\\Qid\\E)","",train_dtm$id))
train_dtm %>% cast_dtm(id,Words,n)
train_dtm
train_dtm %>% cast_dtm(id,Words,n)
ap_td <- tidy(AssociatedPress)
# the output here ap_td should look very very similar to your tibble above
ap_tdm %>% cast_dtm(document, term, count)
data("AssociatedPress", package = "topicmodels")
install.packages("topicmodels")
AssociatedPress
data("AssociatedPress", package = "topicmodels")
AssociatedPress
data("AssociatedPress", package = "topicmodels")
ap_td <- tidy(AssociatedPress)
ap_tdm %>% cast_dtm(document, term, count)
ap_td %>% cast_dtm(document, term, count)
ap_td
train_dtm
train_dtm[-1] %>% cast_dtm(id,Words,n)
train_dtm[-1] %>% cast_dtm(id,term,n)
train_dtm$id = as.integer(gsub("(\\Qid\\E)","",train_dtm$id))
train_dtm[-1]
train_dtm[,-1]
train_dtm[,-1] %>% cast_dtm(id,term,n)
train_dtm[,-1] %>% cast_dtm(id,Words,n)
ap_td
r
train_dtm = train_dtm[,-1]
colnames(train_dtm) = c("document", "term", "count")
train_dtm
trainOrig$id %>% cast_dtm(document, term, count)
train_dtm %>% cast_dtm(document, term, count)
View(testMutated)
rm(ap_td)
rm(df)
View(testMutated)
View(trainMutated)
class(trainMutated)
tidy(trainMutated)
library(tidyverse)
library(DT)
# library('wordcloud')
# library(igraph)
# library(ggraph)
#
library(caret)
library(dtplyr)
library(dplyr)
library(data.table)
library(ggplot2)
###for text analytics
library(koRpus)
library(SnowballC)
library(tidytext)
library(stringr)
library(stringi)
library(tm)
library(wordnet)
library(wordcloud)
tidy(trainMutated)
trainMutated %>% head()
trainMutated = tidy(trainMutated)
trainMutated = trainOrig %>% unnest_tokens(Words,text)
class(trainMutated)
is.tibble(trainMutated)
is.table(trainMutated)
as.table(trainMutated)
trainMutated
as.tibble(trainMutated)
trainMutated = trainMutated %>%
anti_join(stop_words,by = c("Words" = "word"))
trainMutated = trainMutated %>% as.tibble() %>%
anti_join(stop_words,by = c("Words" = "word"))
trainMutated
trainMutated[,-c("len","Words")]
testDf = trainMutated %>% as.data.frame()
testDf[,-c("len","Words")]
testDf[,-c("len")]
trainOrig[,-c("len")] %>% head(1000)
trainOrig
class(trainOrig)
trainOrig[,-c("len","Words")]
trainOrig[,-c("len","id")]
class(testDf)
testDf[, -c("len")]
trainMutated %>% select(!len)
trainMutated
trainMutated %>% select(!c("len"))
trainMutated %>% select(len)
as.tibble(trainMutated)
trainOrig[,-c("len")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble()
mutate(Words = stemDocument(Words)) %>%
group_by(author,id,Words) %>% summarise(n = n()) %>% ungroup()
trainOrig[,-c("len")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(author,id,Words) %>% summarise(n = n()) %>% ungroup()
train_dtm <- trainOrig[,-c("len")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(author,id,Words) %>% summarise(n = n()) %>% ungroup()
train_dtm
trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(author,id,Words) %>% summarise(n = n()) %>% ungroup()
trainOrig
trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(author,id,Words) %>% summarise(n = n()) %>% ungroup()
trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(id,Words) %>% summarise(n = n()) %>% ungroup()
train_dtm <- trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(id,Words) %>% summarise(n = n()) %>% ungroup()
tidy(train_dtm)
train_dtm %>% cast_dtm(id, Words, n)
mtcars
mtcars[,-c("hp","wt")]
mtcars[,-c(hp,wt)]
mtcars[-c(hp,wt)]
mtcars[-c("hp","wt")]
mtcars[hp]
mtcars["hp"]
mtcars[-c(1,2,3)]
mtcars[-c("hp",2,3)]
trainMutated = trainMutated %>% as.tibble() %>%
anti_join(stop_words,by = c("Words" = "word"))
edit(testDf)
train_dtm <- trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(id,Words) %>% summarise(n = n()) %>% ungroup()
train_dtm %>% cast_dtm(id,Words,n)
train_dtm
castDTM = train_dtm %>% cast_dtm(id,Words,n)
castDTM
View(cast_dtm())
View(castDTM)
as.matrix(castDTM)
trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(id,Words) %>% summarise(n = n())
rain_dtm <- trainOrig[,-c("len","author")] %>% head(1000) %>%
unnest_tokens(Words, text) %>% as.tibble() %>%
mutate(Words = stemDocument(Words)) %>%
group_by(id,Words) %>% summarise(n = n())
castDTM = train_dtm %>% cast_dtm(id,Words,n)
castDTM
trainOrig[,-c("len","author")]
class(trainOrig)
class(trainMutated)
